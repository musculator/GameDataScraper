{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "outdir = './partialdata'\n",
    "baseUrl = 'https://www.mobygames.com/browse/games/offset,%s/so,0a/list-games/'\n",
    "pageSize = 25\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that downloads a url\n",
    "def DownloadWebPage(url):       \n",
    "    print('Downloading: ' + url)\n",
    "    time.sleep(1)\n",
    "    response = get(url, headers=headers)\n",
    "    return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#Function that downloads the specified page number\n",
    "def DownloadNextPage(currentPage):       \n",
    "    nextUrl = baseUrl % (currentPage * pageSize)\n",
    "    return DownloadWebPage(nextUrl)\n",
    "\n",
    "#Function that gets all the game links from an html\n",
    "def GetGameLinks(html_soup):\n",
    "    currentPapeGameLinks = []\n",
    "    gamesLinks = html_soup.select('table#mof_object_list tbody tr td:first-child a')\n",
    "    lastPage = len(gamesLinks) < pageSize\n",
    "\n",
    "    for gameLink in gamesLinks:\n",
    "        currentPapeGameLinks.append(gameLink['href'])\n",
    "    return currentPapeGameLinks, lastPage\n",
    "\n",
    "#Function that gets the data of the games from a list of pages\n",
    "def GetGamesData(gameLinks):\n",
    "    df = pd.DataFrame()\n",
    "    for gameLink in gameLinks:    \n",
    "        html_soup = DownloadWebPage(gameLink)\n",
    "        try:\n",
    "            tittle = html_soup.find('h1', class_='niceHeaderTitle').a.text\n",
    "            game = {'GameTittle': tittle}\n",
    "\n",
    "            gameData = html_soup.select('div#coreGameRelease div')\n",
    "            for attributeName, attributeValue in zip(*[iter(gameData)]*2):\n",
    "                game[attributeName.text] = attributeValue.text\n",
    "\n",
    "            gameData = html_soup.select('div#coreGameGenre div div')\n",
    "            for attributeName, attributeValue in zip(*[iter(gameData)]*2):\n",
    "                game[attributeName.text] = attributeValue.text\n",
    "            df = df.append(game, ignore_index=True)\n",
    "        except:\n",
    "            game = {'GameTittle': ''}\n",
    "            df = df.append(game, ignore_index=True)\n",
    "        print('Game processed: ' + tittle)\n",
    "    return df\n",
    "\n",
    "#Function that unifies all temporary files in a single csv\n",
    "def JoinFiles():\n",
    "    df = pd.DataFrame()\n",
    "    files = os.listdir(outdir)\n",
    "    for file in files:\n",
    "        dftmp = pd.read_csv(os.path.join(outdir,file))\n",
    "        df = df.append(dftmp, ignore_index=True)\n",
    "    df.to_csv ('export_dataframe.csv', index = False, header=True)   \n",
    "    \n",
    "#Main function that downloads the game data from the mobygames page, \n",
    "#indicating the initial page, and the number of pages it will store in each temporary file\n",
    "def GameScrapping(maxPagesInFile = 50, currentPage = 0):    \n",
    "    pagesInFile = 0\n",
    "    gameLinks = []\n",
    "    lastPage = False\n",
    "    \n",
    "    start_time = time.time()\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "\n",
    "    while not lastPage:        \n",
    "        html_soup = DownloadNextPage(currentPage)\n",
    "        currentPapeGameLinks,lastPage = GetGameLinks(html_soup)\n",
    "        print('Add %s links' % (len(currentPapeGameLinks)))   \n",
    "        gameLinks += currentPapeGameLinks\n",
    "        pagesInFile += 1\n",
    "        if pagesInFile == maxPagesInFile:\n",
    "            print('Total %s links' % (len(gameLinks)))   \n",
    "            df = GetGamesData(gameLinks)\n",
    "            df.to_csv ('%s\\export_dataframe_%s.csv' % (outdir,currentPage), index = False, header=True)        \n",
    "            pagesInFile = 0\n",
    "            gameLinks = []\n",
    "            lastPage=True\n",
    "        currentPage += 1\n",
    "\n",
    "    print('Total %s links' % (len(gameLinks)))   \n",
    "    if len(gameLinks) > 0:\n",
    "        df = GetGamesData(gameLinks)    \n",
    "        df.to_csv ('%s\\export_dataframe_%s.csv' % (outdir,currentPage), index = False, header=True)   \n",
    "\n",
    "    JoinFiles()\n",
    "\n",
    "    print(\"Total time %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download all games starting from page 0 and saving 50 pages in each file\n",
    "GameScrapping(maxPagesInFile = 1, currentPage = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Platforms'].str.get_dummies(',')\n",
    "#df[df['Add-on'].notnull()]\n",
    "#df.isnull().sum()\n",
    "#dftest = pd.read_csv('export_dataframe.csv')\n",
    "#print(dftest.shape)\n",
    "#dftest.isnull().sum()\n",
    "#dftest['Add-on'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
